{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher forcing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  3 10:36:26 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 27%   32C    P8     8W / 250W |    101MiB / 11016MiB |     13%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| 38%   65C    P3    53W / 250W |      1MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1046      G   /usr/lib/xorg/Xorg                            99MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5809261071221456325\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 684667629247630923\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10812368487\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15658543298815003355\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13749953680598633488\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1' #用0這顆比較不會跟別人打架\n",
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='false'\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.allow_soft_placement=True\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@author: charlie\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding , Input , Dense, Flatten , Activation ,\\\n",
    "                        GRU, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('post-response_100000_8words.pkl' , 'rb' ) as f:\n",
    "    post, response  = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self , all_train_list, seq_len = 10):\n",
    "        \"\"\"traininig data only\n",
    "        input list: post + response\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len \n",
    "        \n",
    "        self.word_uniq = {'EOS','BOS','OTHER'}\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        \n",
    "        for line in all_train_list:\n",
    "            if len(line)==0:\n",
    "                continue\n",
    "                \n",
    "            self.word_uniq |= set(line)\n",
    "        self.word_uniq = sorted(list(self.word_uniq))\n",
    "        self.num_word = len(self.word_uniq)\n",
    "        for i,word in enumerate(self.word_uniq):\n",
    "            self.word2id[word] =i+1\n",
    "        \n",
    "        self.word2id['PAD']=0\n",
    "        self.PAD= self.word2id['PAD']\n",
    "        self.OTHER=self.word2id['OTHER']\n",
    "        self.EOS = self.word2id['EOS']\n",
    "        self.BOS = self.word2id['BOS']\n",
    "        \n",
    "        for k,v in self.word2id.items():\n",
    "            self.id2word[v]=k\n",
    "        \n",
    "    \n",
    "    def token(self,  sents):\n",
    "        \"\"\" tokenize & padding \n",
    "        input/output: list of sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        tokened = []\n",
    "        for line in sents:\n",
    "            tmp = []\n",
    "            for word in line:\n",
    "                tmp.append(self.word2id.get(word , self.OTHER))\n",
    "            tmp.append(self.EOS)\n",
    "            tmp = tmp if len(tmp)<= self.seq_len else tmp[:self.seq_len-1]+[self.EOS]\n",
    "            while len(tmp) <self.seq_len:\n",
    "                tmp.append(self.PAD)\n",
    "            tokened.append(tmp)\n",
    "        return tokened\n",
    "    def detoken(self, sents, space_unused_token=False):\n",
    "        \"\"\" de-tokenize \n",
    "        input/output: list of sentence\n",
    "        \"\"\"\n",
    "        detokened = []\n",
    "        for line in sents:\n",
    "            tmp = []\n",
    "            for token in line:\n",
    "                if space_unused_token and token in {self.OTHER,\n",
    "                                                    self.EOS,\n",
    "                                                    self.BOS,\n",
    "                                                    self.PAD }:\n",
    "                    continue\n",
    "                tmp.append(self.id2word.get(token , ''))\n",
    "            detokened.append(tmp)\n",
    "        return detokened\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post), len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len of post and response\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('max len of post and response')\n",
    "max([len(line)  for line in post]) , max([len(line)  for line in  response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=10\n",
    "post , response = [sent if len(sent)<=seq_len+1 else sent[:seq_len+1] for sent in post] , \\\n",
    "                  [sent if len(sent)<=seq_len+1 else sent[:seq_len+1] for sent in response]\n",
    "\n",
    "post_train , post_test = post[:50000] , post[50000:]\n",
    "response_train , response_test = response[:50000] , response[50000:]\n",
    "\n",
    "tokenizer = Tokenizer(post_train+response_train , seq_len = seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_word: 49607\n"
     ]
    }
   ],
   "source": [
    "print(f'num_word: {tokenizer.num_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverted sentence\n",
      "['谁', '说', '宅男', '战力', '不如', '鹅', '来的', '？', 'EOS', 'PAD']\n",
      "['OTHER', '。', '右', '一', '为', '毛', '主席', '。', 'EOS', 'PAD']\n",
      "['摄影师们', '平时', '一定', '要', '注意', '加强', '身体', '锻炼', 'EOS', 'PAD']\n",
      "\n",
      "original sentence\n",
      "['谁', '说', '宅男', '战力', '不如', '鹅', '来的', '？']\n",
      "['毛家', '。', '右', '一', '为', '毛', '主席', '。']\n",
      "['摄影师们', '平时', '一定', '要', '注意', '加强', '身体', '锻炼']\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "\n",
    "print('reverted sentence')\n",
    "for l in tokenizer.detoken(tokenizer.token(post_test[:3]),False): \n",
    "    #  include padded tokens if False\n",
    "    print(l)\n",
    "    \n",
    "print('\\noriginal sentence')\n",
    "for l in  post_test[:3]:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 ,x2 , y = [],[],[]\n",
    "\n",
    "for line  in tokenizer.token(post):\n",
    "    x1.append(line)\n",
    "    \n",
    "for line in tokenizer.token(response):\n",
    "    teacher = line if line[-1] != tokenizer.EOS else line[:-2]+[tokenizer.EOS, tokenizer.PAD]\n",
    "    x2.append([tokenizer.BOS]+teacher[:-1])\n",
    "    y.append(line)\n",
    "x1,x2,y = np.array(x1) , np.array(x2) , np.array(y).reshape(-1 ,seq_len , 1)\n",
    "x1_train , x1_test = x1[:50000 ] , x1[50000: ]\n",
    "x2_train , x2_test = x2[:50000] , x2[50000:]\n",
    "y_train ,y_test = y[:50000] , y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10, 128)      6349824     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 10, 256)      394240      embedding[1][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 49608)    12749256    lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 19,887,560\n",
      "Trainable params: 19,887,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_model(num_word = tokenizer.num_word ,\n",
    "              seq_len = tokenizer.seq_len, \n",
    "              nunit=256 , \n",
    "              embedding_dim=128 ):\n",
    "\n",
    "    # declare\n",
    "    input_x       =Input(shape=(seq_len ,))\n",
    "    input_teacher = Input(shape=(seq_len , ))\n",
    "    embedding     = Embedding(num_word+1, embedding_dim)    # token start from 1 and including PAD\n",
    "    encoder = LSTM(nunit,return_state=True) \n",
    "    decoder = LSTM(nunit,return_sequences = True)\n",
    "    logit = Dense(num_word+1,activation='softmax')\n",
    "    \n",
    "    \n",
    "    # inference\n",
    "    emb_x       = embedding(input_x)\n",
    "    emb_teacher = embedding(input_teacher)\n",
    "    latent_enc ,h,c  = encoder(emb_x)\n",
    "    latent_dec = decoder(emb_teacher , initial_state=[h,c])\n",
    "    prob = logit(latent_dec)\n",
    "    \n",
    "    model = Model(inputs=[input_x, input_teacher] , outputs = prob)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "earlystop = EarlyStopping(monitor='loss',patience = 10 , verbose=2, mode='auto')\n",
    "model = get_model()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default adam\n",
    "# learning_rate=0.001,\n",
    "#                beta_1=0.9,\n",
    "#                beta_2=0.999,\n",
    "#                epsilon=1e-7,\n",
    "#                amsgrad=False,\n",
    "#                name='Adam',\n",
    "\n",
    "# optim = tf.keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer='adagrad',\n",
    "#              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1_tensor=tf.convert_to_tensor(x1[:100])\n",
    "# x2_tensor=tf.convert_to_tensor(x2[:100])\n",
    "# y_tensor = tf.convert_to_tensor(y[:100])\n",
    "\n",
    "# x1_tensor=np.array(x1)\n",
    "# x2_tensor=np.array(x2)\n",
    "# y_tensor = np.array(y).reshape(-1 , 40 , 1)\n",
    "\n",
    "\n",
    "train_size = 'all'\n",
    "\n",
    "if train_size =='all':\n",
    "    x1_part , x2_part , y_part = x1_train , x2_train , y_train\n",
    "else:\n",
    "    x1_part , x2_part , y_part = x1_train[:train_size] , x2_train[:train_size] , y_train[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 - 95s - loss: 5.6802 - acc: 0.2945 - val_loss: 5.1489 - val_acc: 0.3487\n",
      "Epoch 2/100\n",
      "50000/50000 - 84s - loss: 4.9619 - acc: 0.3546 - val_loss: 5.0554 - val_acc: 0.3606\n",
      "Epoch 3/100\n",
      "50000/50000 - 74s - loss: 4.7628 - acc: 0.3713 - val_loss: 5.0026 - val_acc: 0.3691\n",
      "Epoch 4/100\n",
      "50000/50000 - 75s - loss: 4.6019 - acc: 0.3857 - val_loss: 4.9724 - val_acc: 0.3729\n",
      "Epoch 5/100\n",
      "50000/50000 - 75s - loss: 4.1895 - acc: 0.4192 - val_loss: 4.9503 - val_acc: 0.3802\n",
      "Epoch 8/100\n",
      "50000/50000 - 76s - loss: 4.0634 - acc: 0.4290 - val_loss: 4.9656 - val_acc: 0.3820\n",
      "Epoch 9/100\n",
      "50000/50000 - 76s - loss: 3.9400 - acc: 0.4379 - val_loss: 4.9776 - val_acc: 0.3819\n",
      "Epoch 10/100\n",
      "50000/50000 - 75s - loss: 3.8188 - acc: 0.4461 - val_loss: 5.0056 - val_acc: 0.3826\n",
      "Epoch 11/100\n",
      "50000/50000 - 73s - loss: 3.6997 - acc: 0.4535 - val_loss: 5.0297 - val_acc: 0.3831\n",
      "Epoch 12/100\n",
      "50000/50000 - 73s - loss: 3.5828 - acc: 0.4614 - val_loss: 5.0597 - val_acc: 0.3844\n",
      "Epoch 13/100\n",
      "50000/50000 - 73s - loss: 3.4668 - acc: 0.4705 - val_loss: 5.1007 - val_acc: 0.3830\n",
      "Epoch 14/100\n"
     ]
    }
   ],
   "source": [
    "Hist=model.fit([x1_part,x2_part] , y_part,\n",
    "                validation_data=([x1_test, x2_test],y_test),\n",
    "#                validation_split = .2,\n",
    "                epochs=100,\n",
    "                batch_size = 128,\n",
    "                verbose = 2)\n",
    "#                   callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000000\n"
     ]
    }
   ],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this','is', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print('%2.8f'%score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "print('x1:\\n',x1_train[:2])\n",
    "print('x2:\\n',x2_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = model.predict([x1_train[:10],x2_train[:10]])\n",
    "pred_ids = tf.argmax(pred_prob, axis=2)\n",
    "\n",
    "output = tokenizer.detoken(pred_ids.numpy(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "驰骋乔恩内蒙古转···只是转···只是峯岸PADPAD\n",
      "正宗十一点四十五学文无所从业者转···只是WhenPAD\n",
      "掛墙式转···只是转···只是盆切深乔恩寒舟妖风PAD\n",
      "左眼跳财妖风妖风大世界菊花兰大排球转···只是\n",
      "小半坛入校无所麦粒肿拉米苏转···只是WhenWhenWhenPAD\n",
      "系外无所最恨删博啊转···只是When\n",
      "攻击肥胖转···只是妖风寒舟WhenWhenWhenWhen\n",
      "吃好妖风妖风乔恩乔恩WhenWhen兰大PAD\n",
      "盛夫马三立拳拳之心性感呐今朝PADPADPAD\n",
      "可刷老大帅老大帅心满意足之余乔恩When\n"
     ]
    }
   ],
   "source": [
    "for line in output:\n",
    "    print(''.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王大姐，打字细心一点EOSPADPADPAD\n",
      "慈善再透明，捐款都无意EOSPADPADPAD\n",
      "都说喵是会飞的真不开玩笑EOSPADPAD\n",
      "你就是“反动派”。罗！EOSPAD\n",
      "厨房那个太搞笑了！哈哈哈EOSPADPAD\n",
      "儿童节快乐！我要礼物…EOSPADPAD\n",
      "乐出音了，旁边人直看我EOSPAD\n",
      "少了个新浪微博呀！EOSPADPAD\n",
      "下面在做，上面在看EOSPADPADPADPAD\n",
      "缘分终究会到只不过早晚的事EOSPAD\n"
     ]
    }
   ],
   "source": [
    "for line in tokenizer.detoken(y_train[:10].reshape(-1 , seq_len),False):\n",
    "    print(''.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n"
     ]
    }
   ],
   "source": [
    "def predict_seqs(model ,x1_test, tokenizer= tokenizer):\n",
    "    x2_tmp = np.zeros(x1_test.shape)\n",
    "    x2_tmp[:][0]=tokenizer.BOS\n",
    "    \n",
    "    for i in range(x1_test.shape[1]-1):\n",
    "        \n",
    "        x1_tmp = x1_test[i]\n",
    "        next_tokens = np.argmax(model.predict([x1_test ,x2_tmp])[:,i])\n",
    "        x2_tmp[:,i+1] = next_tokens\n",
    "    \n",
    "    return x2_tmp\n",
    "\n",
    "\n",
    "pred_ids = predict_seqs(model , x1_train[:12])\n",
    "output = tokenizer.detoken(pred_ids,False)\n",
    "for line in output:\n",
    "    print(''.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 115. GiB for an array with shape (50000, 10, 61609) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-e1f6c0bd029e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# tokenizer.num_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# pred_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                     \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 115. GiB for an array with shape (50000, 10, 61609) and data type float32"
     ]
    }
   ],
   "source": [
    "# tokenizer.num_word\n",
    "# pred_ids\n",
    "model.predict([x1_train, x2_train]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 绘制训练 & 验证的准确率值\n",
    "plt.plot(Hist.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(Hist.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# keras.models.load_model\n",
    "model.save('TF_len10_1.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai-lab/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('TF_len10_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
