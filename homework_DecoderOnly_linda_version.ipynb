{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18232472681445650613\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18341764441924178988\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10503562855\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16091096537420782975\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12062879534506948838\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n@author: charlie\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding , Input , Dense, Flatten , Activation ,\\\n",
    "                        GRU, LSTM\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "import pickle \n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1' #用0這顆比較不會跟別人打架\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.allow_soft_placement=True\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU!!')\n",
    "\"\"\"\n",
    "\n",
    "@author: charlie\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlen , alen = 20 ,10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=[],[]\n",
    "post  , response = [] , []\n",
    "\n",
    "\n",
    "with open('qa_dataset/stc_weibo_train_post', 'r' ) as f:\n",
    "    for i in f.readlines():\n",
    "        post.append(i.split())\n",
    "        \n",
    "with open('qa_dataset/stc_weibo_train_response', 'r' ) as f:\n",
    "    for i in f.readlines():\n",
    "        response.append(i.split())\n",
    "\n",
    "for i,po in enumerate(post):\n",
    "    if len(x)>=100000:\n",
    "        break\n",
    "    tmp_polen  , tmp_rsplen = len(po) , len(response[i])\n",
    "    if tmp_polen>=alen  or tmp_rsplen>=alen or tmp_polen<3 or tmp_rsplen <3:\n",
    "        continue\n",
    "    x.append(po)\n",
    "    y.append(response[i])\n",
    "\n",
    "with open('post-response_10thousand_3_9.pkl', 'wb') as f:\n",
    "    pickle.dump([x , y] , f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('post-response_10thousand_3_9.pkl' , 'rb' ) as f:\n",
    "    post, response  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post) , len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self , all_train_list, qlen =20, alen = 10):\n",
    "        \"\"\"traininig data only\n",
    "        input list: post + response\n",
    "        \"\"\"\n",
    "        self.qlen = qlen\n",
    "        self.alen = alen\n",
    "        \n",
    "        self.word_uniq = {'EOS','BOS','OTHER'}\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        \n",
    "        for line in all_train_list:\n",
    "            if len(line)==0:\n",
    "                continue\n",
    "                \n",
    "            self.word_uniq |= set(line)\n",
    "        self.word_uniq = sorted(list(self.word_uniq))\n",
    "        self.num_word = len(self.word_uniq)\n",
    "        for i,word in enumerate(self.word_uniq):\n",
    "            self.word2id[word] =i+1\n",
    "        \n",
    "        self.word2id['PAD']=0\n",
    "        self.PAD= self.word2id['PAD']\n",
    "        self.OTHER=self.word2id['OTHER']\n",
    "        self.EOS = self.word2id['EOS']\n",
    "        self.BOS = self.word2id['BOS']\n",
    "        \n",
    "        for k,v in self.word2id.items():\n",
    "            self.id2word[v]=k\n",
    "        \n",
    "    \n",
    "    def token(self,  sents, source = 'q'):\n",
    "        \"\"\" tokenize & padding \n",
    "        input/output: list of sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = 20 if source=='q' else 10\n",
    "        \n",
    "        tokened = []\n",
    "        for line in sents:\n",
    "            tmp = []\n",
    "            for word in line:\n",
    "                tmp.append(self.word2id.get(word , self.OTHER))\n",
    "            tmp.append(self.EOS)\n",
    "            tmp = tmp if len(tmp)<= seq_len else tmp[:seq_len-1]+[self.EOS]\n",
    "            while len(tmp) <seq_len:\n",
    "                tmp.append(self.PAD)\n",
    "            tokened.append(tmp)\n",
    "        return tokened\n",
    "    def detoken(self, sents, space_unused_token=False):\n",
    "        \"\"\" de-tokenize \n",
    "        input/output: list of sentence\n",
    "        \"\"\"\n",
    "        detokened = []\n",
    "        for line in sents:\n",
    "            tmp = []\n",
    "            for token in line:\n",
    "                if space_unused_token and token in {self.OTHER,\n",
    "                                                    self.EOS,\n",
    "                                                    self.BOS,\n",
    "                                                    self.PAD }:\n",
    "                    continue\n",
    "                tmp.append(self.id2word.get(token , ''))\n",
    "            detokened.append(tmp)\n",
    "        return detokened\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_train , post_test = post[:50000] , post[50000:]\n",
    "response_train , response_test = response[:50000] , response[50000:]\n",
    "\n",
    "tokenizer = Tokenizer(post_train+response_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_word: 52291\n"
     ]
    }
   ],
   "source": [
    "print(f'num_word: {tokenizer.num_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x , y = [],[]\n",
    "x_test , y_test = [] , []\n",
    "\n",
    "for line  in tokenizer.token(post_train):\n",
    "    x.append(line)\n",
    "    \n",
    "for line in tokenizer.token(response_train):\n",
    "    y.append(line)\n",
    "\n",
    "for line  in tokenizer.token(post_test):\n",
    "    x_test.append(line)\n",
    "    \n",
    "for line in tokenizer.token(response_test):\n",
    "    y_test.append(line)\n",
    "\n",
    "# x1,x2,y = np.array(x1) , np.array(x2) , np.array(y).reshape(-1 ,seq_len , 1)\n",
    "# x1_train , x1_test = x1[:50000 ] , x1[50000: ]\n",
    "# x2_train , x2_test = x2[:50000] , x2[50000:]\n",
    "# y_train ,y_test = y[:50000] , y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6524,\n",
       "  38985,\n",
       "  43345,\n",
       "  43951,\n",
       "  13559,\n",
       "  26485,\n",
       "  2789,\n",
       "  1443,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [8998,\n",
       "  44406,\n",
       "  46834,\n",
       "  37370,\n",
       "  16082,\n",
       "  37294,\n",
       "  31326,\n",
       "  14704,\n",
       "  52078,\n",
       "  1443,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [8127,\n",
       "  41819,\n",
       "  30312,\n",
       "  23030,\n",
       "  37294,\n",
       "  50113,\n",
       "  35555,\n",
       "  34369,\n",
       "  46912,\n",
       "  1443,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中国', '移动', '营销', '行来', '发展', '报告', 'alink', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['你', '见', '过', '皮卡丘', '喝水', '的', '样子', '吗', '？', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "['从', '胚胎', '期', '开始', '的', '面部', '特征', '演变', '过程', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "for line in tokenizer.detoken(x[:3]):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_model(nword = tokenizer.num_word , nunit=128 , embedding_dim=128 ):\n",
    "\n",
    "    # declare\n",
    "    input_x       =Input(shape=(tokenizer.qlen ,))\n",
    "    input_teacher = Input(shape=(tokenizer.alen , ))\n",
    "    embedding     = Embedding(nword+1, embedding_dim)    # token start from 1 and including PAD\n",
    "    encoder = LSTM(nunit,return_state=True) \n",
    "    decoder = LSTM(nunit,return_sequences = True)\n",
    "    lin = Dense(nword+1,activation='softmax')\n",
    "    \n",
    "    \n",
    "    # inference\n",
    "    emb_x       = embedding(input_x)\n",
    "    emb_teacher = embedding(input_teacher)\n",
    "    latent_enc ,h,c  = encoder(emb_x)\n",
    "    latent_dec = decoder(emb_teacher , initial_state=[h,c])\n",
    "    prob = lin(latent_dec)\n",
    "    \n",
    "    model = Model(inputs=[input_x, input_teacher] , outputs = prob)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "earlystop = EarlyStopping(monitor='loss',patience = 2 , verbose=2, mode='auto')\n",
    "model = get_model()\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['acc'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
